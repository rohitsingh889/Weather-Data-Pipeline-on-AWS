# ğŸŒ¦ Incremental Weather Data Lake Pipeline on AWS

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/AWS%20pipeline%20main%20architecture.png)
This project implements a production-style **batch data engineering pipeline** that ingests historical weather data from the **Open-Meteo Archive API**, stores immutable raw data in Amazon S3, performs distributed transformations using **AWS Glue (PySpark)**, generates analytics-ready datasets, and enables SQL-based querying via **AWS Glue Data Catalog and Amazon Athena** â€” fully orchestrated by **Apache Airflow in a Dockerized local environment**.

**Data Source API:** https://open-meteo.com/

---

## ğŸš€ Project Objective

The goal of this pipeline is to demonstrate **real-world cloud data engineering patterns**, including:

âœ” Workflow orchestration  
âœ” API-driven ingestion  
âœ” Multi-layer data lake architecture  
âœ” Distributed Spark transformations  
âœ” Incremental processing strategy  
âœ” Data quality validation  
âœ” Metadata-driven analytics  
âœ” BI consumption workflows  

---

## ğŸ— High-Level Architecture

Pipeline Flow:

Open-Meteo Archive API  
â†’ Python Extraction Layer  
â†’ Amazon S3 (Bronze Layer â€“ Raw JSON)  
â†’ AWS Glue (Silver Layer â€“ PySpark Transformations + Data Quality Checks)  
â†’ Amazon S3 (Silver Layer â€“ Parquet)  
â†’ AWS Glue (Gold Layer â€“ Aggregations)  
â†’ Amazon S3 (Gold Layer â€“ Analytics-Ready Parquet)  
â†’ AWS Glue Crawler  
â†’ AWS Glue Data Catalog  
â†’ Amazon Athena  
â†’ BI / Analytics Tools (Power BI)

---

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/pipelineinfo.png)

---
---

## ğŸ§© Technologies & Tools Used

| Category | Technology / Tool | Purpose |
|----------|-------------------|---------|
| Orchestration | Apache Airflow (Dockerized) | Workflow scheduling & dependency management |
| Containerization | Docker | Local Airflow environment isolation |
| Language | Python | API ingestion & orchestration logic |
| HTTP Client | Requests Module | REST API communication |
| AWS SDK | Boto3 | Programmatic AWS interaction |
| Cloud Storage | Amazon S3 | Data lake storage layers |
| Distributed Processing | AWS Glue (PySpark) | Spark-based ETL transformations |
| Metadata Discovery | AWS Glue Crawler | Schema inference & partition detection |
| Metadata Catalog | AWS Glue Data Catalog | Athena table definitions |
| Query Engine | Amazon Athena | SQL querying on S3 |
| BI Tool | Power BI | Analytics & visualization |
| Credential Management | AWS CLI + IAM Roles | Secure AWS authentication |

---


## ğŸ”„ End-to-End Data Lifecycle (Step-Wise)

The pipeline follows a structured **Extract â†’ Transform â†’ Load â†’ Analytics** workflow.

---

### 1ï¸âƒ£ Extract Phase â€“ API Ingestion

âœ” Weather data retrieved from Open-Meteo Archive API  
âœ” Python used as the ingestion engine  
âœ” REST calls executed via the `requests` module  
âœ” Raw JSON responses preserved  

**Output:**

â†’ Raw JSON stored in Bronze layer (Amazon S3)

**S3 Folders**

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/s3%20bucket.png)

**Bronze Layer**

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/bronze.png)

---

### 2ï¸âƒ£ Transform Phase â€“ Silver Processing (AWS Glue)

âœ” Partition-specific Bronze data read  
âœ” Nested arrays flattened  
âœ” Timestamps parsed  
âœ” Data types standardized  
âœ” Duplicate records removed  
âœ” Data quality checks enforced  

**Output:**

â†’ Cleaned Parquet datasets written to Silver layer

**Silver Layer**

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/silver.png)

---

### 3ï¸âƒ£ Load Phase â€“ Gold Aggregation (AWS Glue)

âœ” Silver datasets aggregated into daily metrics  
âœ” Business-friendly schema produced  
âœ” Dataset optimized for analytics  

**Output:**

â†’ Analytics-ready Parquet datasets written to Gold layer

**Gold Layer**

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/gold.png)

---

### 4ï¸âƒ£ Analytics Phase â€“ Query & BI Consumption

âœ” Glue Crawler infers schema  
âœ” Glue Data Catalog stores metadata  
âœ” Athena executes SQL queries  
âœ” Power BI consumes Gold datasets  

**Purpose:**

â†’ Business intelligence & reporting

---




## ğŸ“¡ Data Source Layer

**Provider:** Open-Meteo Archive API  

The pipeline retrieves **historical hourly weather observations** for configured cities.

Data retrieved includes:

- temperature_2m  
- precipitation  
- windspeed_10m  
- timestamp  

The API returns nested JSON structures requiring flattening.

---

## âš™ Extraction Layer (Python)

### Responsibilities

âœ” Call Open-Meteo API using `requests`  
âœ” Fetch previous dayâ€™s data dynamically  
âœ” Preserve raw JSON response  
âœ” Upload directly to S3 Bronze layer using `boto3`

All source Python scripts are stored in the **same configured DAG location inside the Dockerized Airflow environment**, ensuring consistent execution and simplified orchestration.

---

### Libraries Used

**Requests Module**

Used for external REST API communication.

**Boto3 (AWS SDK)**

Used for:

âœ” S3 PutObject operations  
âœ” IAM-based authentication  
âœ” Secure AWS integration  

No AWS keys are hardcoded.

---

### AWS Credential Verification

Authentication handled via:

âœ” AWS CLI configuration  
âœ” IAM user / role permissions  
âœ” Boto3 credential provider chain  

Driver logic:

- Boto3 automatically resolves credentials  
- Uses environment / IAM / config chain  
- No manual secret handling required  

---

## ğŸ—‚ Bronze Layer â€“ Raw Data Zone

**Storage:** Amazon S3  
**Format:** Raw JSON  

**Partitioning Strategy:**

```
bronze/weather/
    city=XYZ/
        year=YYYY/
            month=MM/
                day=DD/
```

### Purpose

âœ” Immutable raw storage  
âœ” Replay capability  
âœ” Debugging & auditing  
âœ” Schema recovery  

No transformations applied.

---

## ğŸ”„ Silver Layer â€“ Transformation & Validation Zone

**Processing Engine:** AWS Glue (PySpark)

### Responsibilities

âœ” Read partition-specific Bronze JSON  
âœ” Flatten nested hourly arrays  
âœ” Parse timestamps  
âœ” Cast numeric fields  
âœ” Remove duplicates  
âœ” Apply data quality checks  

---

### Output Format

âœ” Parquet (columnar, optimized)

**Partitioning:**

```
silver/weather/date=YYYY-MM-DD/
```

---

### Data Quality Strategy

Implemented directly in Spark:

âœ” Null validation  
âœ” Domain range validation  
âœ” Duplicate detection  
âœ” Fail-fast enforcement  

Example checks:

- Null timestamps rejected  
- Temperature bounds enforced  
- Negative windspeed prevented  
- Duplicate city/timestamp removed  

Bad data â†’ Job fails intentionally.

---

### ğŸš¦ Data Quality Enforcement Strategy

Silver layer transformations implement a **fail-fast validation model**:

âœ” Null checks  
âœ” Domain range checks  
âœ” Duplicate detection  

Invalid records trigger controlled job failure to prevent downstream corruption.

**Future Enhancement â€“ Quarantine Pattern**

âœ” Redirect invalid rows to `silver/quarantine/`  
âœ” Enable root-cause analysis  
âœ” Preserve pipeline continuity  

**Glue Jobs on AWS**

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/glue%20jobs%20list.png)

**Silver Job Success**

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/silver%20job%20sucess.png)

---

## ğŸ“Š Gold Layer â€“ Analytics Zone

**Processing Engine:** AWS Glue (Aggregation Job)

### Responsibilities

Transform Silver hourly records â†’ Daily metrics

Aggregations:

- avg_temperature  
- max_temperature  
- total_precipitation  
- avg_windspeed  

---

### Output

âœ” Parquet  
âœ” Partitioned by date  

```
gold/weather/date=YYYY-MM-DD/
```

**Gold Job Success**

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/gold%20job%20sucess.png)

---

### Purpose

âœ” BI-ready datasets  
âœ” Reduced scan cost  
âœ” Faster Athena queries  

---

## ğŸ” Incremental Processing Strategy

The pipeline follows a **partition-level incremental model**.

âœ” Process only `process_date`  
âœ” Overwrite only that partition  
âœ” Idempotent reruns  
âœ” Prevent duplicate records  

**Mechanism:**

```python
.mode("overwrite")
.option("replaceWhere", "date = 'YYYY-MM-DD'")
```

---

## â›“ Orchestration Layer â€“ Apache Airflow

Airflow acts as the **central control plane** of the pipeline, coordinating task execution and ensuring reliable workflow management.

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/airflow...dag.png)

### Airflow Responsibilities

âœ” Schedule pipeline runs  
âœ” Manage task dependencies  
âœ” Trigger AWS Glue Jobs  
âœ” Trigger Glue Crawler  
âœ” Retry & failure handling  

**Gantt Chart**

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/Airflow%20gantt%20chart.png)

**Airflow Graph**

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/Airflow%20graph.png)

**Airflow DAG**

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/airflow.png)

---

### Dockerized Airflow Environment

âœ” Local reproducible setup  
âœ” Containerized execution  
âœ” Cloud orchestration simulation  

---

## ğŸ§¾ Metadata & Schema Management

âœ” Glue Crawler infers schema  
âœ” Glue Data Catalog stores tables  

**Glue Crawler**

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/crawler.png)

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/crawler%20sucess.png)

---

## ğŸ” Querying Gold Data with Amazon Athena

Amazon Athena enables **SQL querying directly on S3 Parquet datasets**.

### Why Athena?

âœ” No infrastructure management  
âœ” Pay-per-query pricing  
âœ” Glue Catalog integration  

**Athena Querying**

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/athena%20.png)

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/athena%20tables.png)

---

### Example Query

```sql
SELECT city, avg_temperature
FROM gold_weather
WHERE date = DATE '2026-02-16';
```

---

### Common Analytics Queries

```sql
-- Get weather for a specific date
SELECT city, avg_temperature, max_temperature
FROM gold
WHERE date = '2026-02-16';

-- Daily trend
SELECT date, avg_temperature
FROM gold
WHERE city = 'Delhi'
ORDER BY date;
```

---

## ğŸ“ˆ BI / Visualization Layer â€“ Power BI

âœ” Athena used as SQL backend  
âœ” Gold datasets optimized for dashboards  

Planned visuals:

âœ” Temperature trends  
âœ” City comparisons  
âœ” KPI metrics  

NOTE: BI dashboard upload pending â€” will be completed soon.

---

**VS Code Project Structure**

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/vs%20code%20....png)

---

## ğŸ’° Cost Optimization Strategy

âœ” Parquet columnar format  
âœ” Partitioned datasets  
âœ” Reduced Athena scan costs  

---
## â–¶ï¸ How to Run This Project

This pipeline is designed to run using **Dockerized Apache Airflow locally** while interacting with **AWS services in the cloud**.

---

### 1ï¸âƒ£ Prerequisites

Ensure the following tools are installed:

âœ” Docker  
âœ” Docker Compose  
âœ” AWS CLI  
âœ” Python (optional for local testing)

Verify installations:

```bash
docker --version
docker-compose --version
aws --version
```

---

### 2ï¸âƒ£ Configure AWS Credentials

Authenticate your local environment with AWS:

```bash
aws configure
```

Provide:

âœ” AWS Access Key  
âœ” AWS Secret Key  
âœ” Default Region (e.g., us-east-1)  
âœ” Output Format (json)

Credentials are automatically used by **boto3** and Airflow tasks.

---

### 3ï¸âƒ£ Start Airflow Environment

From the project root directory:

```bash
docker-compose up airflow-init
docker-compose up
```

Airflow services will start inside containers.

Access Airflow UI:

```
http://localhost:8080
```

Default login:

âœ” Username: airflow  
âœ” Password: airflow  

---

### 4ï¸âƒ£ DAG & Script Placement

All pipeline Python scripts (extraction logic, Glue triggers, helpers) must reside inside the **configured DAG folder** mounted into Docker.

Example:

```
project-root/
    dags/
        weather_pipeline_dag.py
        extraction.py
        glue_helpers.py
```

This allows Airflow to automatically discover and execute tasks.

---

### 5ï¸âƒ£ Enable & Trigger Pipeline

Inside Airflow UI:

âœ” Locate the DAG  
âœ” Toggle DAG to "ON"  
âœ” Click **Trigger DAG**

Airflow will execute tasks sequentially:

Extraction â†’ Glue Silver Job â†’ Glue Gold Job â†’ Crawler

---

### 6ï¸âƒ£ Monitor Execution

Airflow provides built-in observability:

âœ” Graph View â†’ Task dependencies  
âœ” Gantt View â†’ Execution timing  
âœ” Logs â†’ Debugging & failures  

---

### 7ï¸âƒ£ Query Results

After successful execution:

âœ” Open Amazon Athena  
âœ” Query `gold` table  

Example:

```sql
SELECT city, avg_temperature
FROM gold
ORDER BY avg_temperature DESC;
```

---

### 8ï¸âƒ£ Failure Recovery

The pipeline supports safe reruns:

âœ” Incremental partition overwrite  
âœ” Idempotent job design  
âœ” No duplicate record risk  

Simply re-trigger the DAG if needed.

---


## ğŸ‘¨â€ğŸ’» Author & Project Context

**Rohit Raj Singh**

This project is part of my professional portfolio and demonstrates a **production-grade cloud data engineering pipeline** using **Apache Airflow and AWS**.

Key skills reflected:

- Workflow orchestration with Apache Airflow (local, Dockerized)  
- REST API ingestion and immutable data lake design  
- AWS Glueâ€“based distributed ETL using PySpark  
- Schema inference and partition management with Glue Crawlers  
- SQL analytics using Amazon Athena  
- Secure AWS integration using boto3 and AWS CLI  
- End-to-end pipeline automation and monitoring  

ğŸ“¬ **LinkedIn:**  
https://www.linkedin.com/in/rohit-raj-singh-3030172a4

---
