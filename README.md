# ğŸŒ¦ Incremental Weather Data Lake Pipeline on AWS

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/AWS%20pipeline%20main%20architecture.png)

This project implements a production-style **batch data engineering pipeline** that ingests historical weather data from the **Open-Meteo Archive API**, stores immutable raw data in Amazon S3, performs distributed transformations using **AWS Glue (PySpark)**, generates analytics-ready datasets, and enables SQL-based querying via **AWS Glue Data Catalog and Amazon Athena** â€” fully orchestrated by **Apache Airflow in a Dockerized local environment**.

**Data Source API:** https://open-meteo.com/

---

## ğŸš€ Project Objective

The goal of this pipeline is to demonstrate **real-world cloud data engineering patterns**, including:

âœ” Workflow orchestration  
âœ” API-driven ingestion  
âœ” Multi-layer data lake architecture  
âœ” Distributed Spark transformations  
âœ” Incremental processing strategy  
âœ” Data quality validation  
âœ” Metadata-driven analytics  
âœ” BI consumption workflows  

---

## ğŸ— High-Level Architecture

**Pipeline Flow**

Open-Meteo Archive API  
â†’ Python Extraction Layer  
â†’ Amazon S3 (Bronze Layer â€“ Raw JSON)  
â†’ AWS Glue (Silver Layer â€“ PySpark Transformations + Data Quality Checks)  
â†’ Amazon S3 (Silver Layer â€“ Parquet)  
â†’ AWS Glue (Gold Layer â€“ Aggregations)  
â†’ Amazon S3 (Gold Layer â€“ Analytics-Ready Parquet)  
â†’ AWS Glue Crawler  
â†’ AWS Glue Data Catalog  
â†’ Amazon Athena  
â†’ BI / Analytics Tools (Power BI)

---

![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/pipelineinfo.png)

---

## ğŸ§© Technologies & Tools Used

| Category | Technology / Tool | Purpose |
|----------|-------------------|---------|
| Orchestration | Apache Airflow (Dockerized) | Workflow scheduling & dependency management |
| Containerization | Docker | Local Airflow environment isolation |
| Language | Python | API ingestion & orchestration logic |
| HTTP Client | Requests Module | REST API communication |
| AWS SDK | Boto3 | Programmatic AWS interaction |
| Cloud Storage | Amazon S3 | Data lake storage layers |
| Distributed Processing | AWS Glue (PySpark) | Spark-based ETL transformations |
| Metadata Discovery | AWS Glue Crawler | Schema inference & partition detection |
| Metadata Catalog | AWS Glue Data Catalog | Athena table definitions |
| Query Engine | Amazon Athena | SQL querying on S3 |
| BI Tool | Power BI | Analytics & visualization |
| Credential Management | AWS CLI + IAM Roles | Secure AWS authentication |

---

## ğŸ”„ End-to-End Data Lifecycle (Step-Wise)

The pipeline follows a structured **Extract â†’ Transform â†’ Load â†’ Analytics** workflow.

---

### 1ï¸âƒ£ Extract Phase â€“ API Ingestion

âœ” Weather data retrieved from the Open-Meteo Archive API  
âœ” Python used as the ingestion engine  
âœ” REST calls executed via the `requests` module  
âœ” Raw JSON responses preserved  

**Output**

â†’ Raw JSON stored in the Bronze layer (Amazon S3)

![S3 Bucket](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/s3%20bucket.png)

![Bronze Layer](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/bronze.png)

---

### 2ï¸âƒ£ Transform Phase â€“ Silver Processing (AWS Glue)

âœ” Partition-specific Bronze data read  
âœ” Nested arrays flattened  
âœ” Timestamps parsed  
âœ” Data types standardized  
âœ” Duplicate records removed  
âœ” Data quality checks enforced  

**Output**

â†’ Cleaned Parquet datasets written to the Silver layer

![Silver Layer](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/silver.png)

---

### 3ï¸âƒ£ Load Phase â€“ Gold Aggregation (AWS Glue)

âœ” Silver datasets aggregated into daily metrics  
âœ” Business-friendly schema produced  
âœ” Dataset optimized for analytics  

**Output**

â†’ Analytics-ready Parquet datasets written to the Gold layer

![Gold Layer](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/gold.png)

---

### 4ï¸âƒ£ Analytics Phase â€“ Query & BI Consumption

âœ” Glue Crawler infers schema  
âœ” Glue Data Catalog stores metadata  
âœ” Athena executes SQL queries  
âœ” Power BI consumes Gold datasets  

**Purpose**

â†’ Business intelligence & reporting

---

## ğŸ“¡ Data Source Layer

**Provider:** Open-Meteo Archive API  

The pipeline retrieves **historical hourly weather observations** for configured cities.

**Data Retrieved**

- temperature_2m  
- precipitation  
- windspeed_10m  
- timestamp  

The API returns nested JSON structures requiring flattening.

---

## âš™ Extraction Layer (Python)

### Responsibilities

âœ” Call Open-Meteo API using `requests`  
âœ” Fetch previous dayâ€™s data dynamically  
âœ” Preserve raw JSON responses  
âœ” Upload directly to the S3 Bronze layer using `boto3`

All source Python scripts are stored in the **same configured DAG location inside the Dockerized Airflow environment**, ensuring consistent execution and simplified orchestration.

---

### Libraries Used

**Requests Module** â€“ REST API communication  
**Boto3 (AWS SDK)** â€“ Secure AWS interaction

âœ” S3 PutObject operations  
âœ” IAM-based authentication  
âœ” No hardcoded AWS credentials  

---

## ğŸ” IAM Role & Security Model

Secure access to AWS services is enforced using **IAM roles and least-privilege permissions**.

âœ” Glue jobs execute using an IAM role with scoped S3 + Glue permissions  
âœ” No static credentials embedded in code  
âœ” Airflow interacts with AWS via configured credentials  
âœ” Access controlled through AWS policy-based authorization  

Typical permissions include:

- S3 read/write access to Bronze / Silver / Gold paths  
- Glue job execution permissions  
- Glue crawler execution permissions  
- Athena query access (via catalog)

This design follows **cloud security best practices** and mirrors real production environments.

---

## ğŸ—‚ Bronze Layer â€“ Raw Data Zone

**Storage:** Amazon S3  
**Format:** Raw JSON  

```
bronze/weather/
    city=XYZ/
        year=YYYY/
            month=MM/
                day=DD/
```

âœ” Immutable storage  
âœ” Replay capability  
âœ” No transformations applied  

---

## ğŸ”„ Silver Layer â€“ Transformation & Validation Zone

**Processing Engine:** AWS Glue (PySpark)

âœ” Flatten nested structures  
âœ” Standardize schema  
âœ” Apply data quality checks  

```
silver/weather/date=YYYY-MM-DD/
```

Bad data â†’ Job fails intentionally.

---

## ğŸ“Š Gold Layer â€“ Analytics Zone

**Processing Engine:** AWS Glue (Aggregation Job)

Aggregations:

- avg_temperature  
- max_temperature  
- total_precipitation  
- avg_windspeed  

```
gold/weather/date=YYYY-MM-DD/
```

âœ” BI-ready datasets  
âœ” Optimized for Athena  

---

## ğŸ” Incremental Processing Strategy

âœ” Process only `process_date`  
âœ” Overwrite only target partition  
âœ” Idempotent reruns  

```python
.mode("overwrite")
.option("replaceWhere", "date = 'YYYY-MM-DD'")
```

---

## â›“ Orchestration Layer â€“ Apache Airflow

Airflow acts as the **central control plane**, coordinating task execution and dependency management.

âœ” Schedule runs  
âœ” Trigger Glue jobs  
âœ” Trigger Crawlers  
âœ” Retry & failure handling  

---

## ğŸ” Querying with Amazon Athena

âœ” Serverless SQL on S3  
âœ” No infrastructure management  
âœ” Pay-per-query pricing  

---

## ğŸ“ˆ BI / Visualization Layer â€“ Power BI

âœ” Athena used as SQL backend  
âœ” Gold datasets optimized for dashboards  

NOTE: BI dashboard upload pending â€” will be completed soon.

---

## ğŸ’° Cost Optimization Strategy

âœ” Parquet columnar format  
âœ” Partitioned datasets  
âœ” Reduced Athena scan costs  

---

## ğŸ‘¨â€ğŸ’» Author & Project Context

**Rohit Raj Singh**

Key skills demonstrated:

- Workflow orchestration with Apache Airflow  
- REST API ingestion & data lake design  
- AWS Glue distributed ETL (PySpark)  
- Incremental processing & validation  
- Athena-based analytics  
- IAM-secured AWS integration  

ğŸ“¬ **LinkedIn:**  
https://www.linkedin.com/in/rohit-raj-singh-3030172a4
