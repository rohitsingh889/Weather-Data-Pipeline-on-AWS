# ğŸŒ¦ Incremental Weather Data Lake Pipeline on AWS

This project implements a production-style **batch data engineering pipeline** that ingests historical weather data from the **Open-Meteo Archive API**, stores raw data in Amazon S3, performs distributed transformations using **AWS Glue (PySpark)**, generates analytics-ready datasets, and enables SQL-based querying via **AWS Glue Data Catalog and Amazon Athena** â€” fully orchestrated by **Apache Airflow running in a Dockerized local environment**.

---

## ğŸš€ Project Objective

The goal of this pipeline is to demonstrate **real-world cloud data engineering patterns**, including:

âœ” Workflow orchestration  
âœ” API-driven ingestion  
âœ” Multi-layer data lake architecture  
âœ” Distributed Spark transformations  
âœ” Incremental processing strategy  
âœ” Data quality validation  
âœ” Metadata-driven analytics  
âœ” BI consumption workflows  

---

## ğŸ— High-Level Architecture

Pipeline Flow:

Open-Meteo Archive API  
â†’ Python Extraction Layer  
â†’ Amazon S3 (Bronze Layer â€“ Raw JSON)  
â†’ AWS Glue (Silver Layer â€“ PySpark Transformations + Data Quality Checks)  
â†’ Amazon S3 (Silver Layer â€“ Parquet)  
â†’ AWS Glue (Gold Layer â€“ Aggregations)  
â†’ Amazon S3 (Gold Layer â€“ Analytics Ready Parquet)  
â†’ AWS Glue Crawler  
â†’ AWS Glue Data Catalog  
â†’ Amazon Athena  
â†’ BI / Analytics Tools (Power BI)

---

---

## ğŸ§© Technologies & Tools Used

| Category | Technology / Tool | Purpose |
|----------|-------------------|---------|
| Orchestration | Apache Airflow (Dockerized) | Workflow scheduling & dependency management |
| Containerization | Docker | Local Airflow environment isolation |
| Language | Python | API ingestion & orchestration logic |
| HTTP Client | Requests Module | REST API communication |
| AWS SDK | Boto3 | Programmatic AWS interaction |
| Cloud Storage | Amazon S3 | Data lake storage layers |
| Distributed Processing | AWS Glue (PySpark) | Spark-based ETL transformations |
| Metadata Discovery | AWS Glue Crawler | Schema inference & partition detection |
| Metadata Catalog | AWS Glue Data Catalog | Athena table definitions |
| Query Engine | Amazon Athena | SQL querying on S3 |
| BI Tool | Power BI | Analytics & visualization |
| Credential Management | AWS CLI + IAM Roles | Secure AWS authentication |

---

---

## ğŸ“¡ Data Source Layer

**Provider:** Open-Meteo Archive API  

The pipeline retrieves **historical hourly weather observations** for configured cities.

Data retrieved includes:

- temperature_2m  
- precipitation  
- windspeed_10m  
- timestamp  

The API returns nested JSON structures requiring flattening.

---

---

## âš™ Extraction Layer (Python)

### Responsibilities

âœ” Call Open-Meteo API using `requests`  
âœ” Fetch previous day's data dynamically  
âœ” Preserve raw JSON response  
âœ” Upload directly to S3 Bronze layer using `boto3`

---

### Libraries Used

**Requests Module**

Used for external REST API communication.

**Boto3 (AWS SDK)**

Used for:

âœ” S3 PutObject operations  
âœ” IAM-based authentication  
âœ” Secure AWS integration  

No AWS keys hardcoded.

---

### AWS Credential Verification

Authentication handled via:

âœ” AWS CLI configuration  
âœ” IAM user / role permissions  
âœ” Boto3 credential provider chain  

Driver logic:

- Boto3 automatically resolves credentials  
- Uses environment / IAM / config chain  
- No manual secret handling required  

---

---

## ğŸ—‚ Bronze Layer â€“ Raw Data Zone

**Storage:** Amazon S3  
**Format:** Raw JSON  
**Partitioning Strategy:**

```
bronze/weather/
    city=XYZ/
        year=YYYY/
            month=MM/
                day=DD/
```

---

### Purpose

âœ” Immutable raw storage  
âœ” Replay capability  
âœ” Debugging & auditing  
âœ” Schema recovery  

No transformations applied.

---

---

## ğŸ”„ Silver Layer â€“ Transformation & Validation Zone

**Processing Engine:** AWS Glue (PySpark)

---

### Responsibilities

âœ” Read partition-specific Bronze JSON  
âœ” Flatten nested hourly arrays  
âœ” Parse timestamps  
âœ” Cast numeric fields  
âœ” Remove duplicates  
âœ” Apply Data Quality Checks  

---

### Output Format

âœ” Parquet (Columnar, optimized)

Partitioning:

```
silver/weather/date=YYYY-MM-DD/
```

---

### Data Quality Strategy

Implemented directly in Spark:

âœ” Null validation  
âœ” Domain range validation  
âœ” Duplicate detection  
âœ” Fail-fast enforcement  

Example checks:

- Null timestamps rejected  
- Temperature bounds enforced  
- Negative windspeed prevented  
- Duplicate city/timestamp removed  

Bad data â†’ Job fails intentionally.

---

---

---

### ğŸš¦ Data Quality Enforcement Strategy

Silver layer transformations implement a **fail-fast validation model**:

âœ” Null checks  
âœ” Domain range checks  
âœ” Duplicate detection  

Invalid records trigger controlled job failure to prevent downstream corruption.

**Future Enhancement â€“ Quarantine Pattern**

In production-grade systems, invalid records are often redirected to a **quarantine / dead-letter zone** for inspection rather than failing the entire job.

Planned extension:

âœ” Redirect invalid rows to `silver/quarantine/`  
âœ” Enable root-cause analysis  
âœ” Preserve pipeline continuity  

Current implementation prioritizes correctness and data integrity.

## ğŸ“Š Gold Layer â€“ Analytics Zone

**Processing Engine:** AWS Glue (Aggregation Job)

---

### Responsibilities

Transform Silver hourly records â†’ Daily metrics

Aggregations:

- avg_temperature  
- max_temperature  
- total_precipitation  
- avg_windspeed  

---

### Output

âœ” Parquet  
âœ” Partitioned by date  

```
gold/weather/date=YYYY-MM-DD/
```

---

### Purpose

âœ” BI-ready datasets  
âœ” Reduced scan cost  
âœ” Faster Athena queries  

---

---

## ğŸ” Incremental Processing Strategy

The pipeline follows a **partition-level incremental model**.

Behavior:

âœ” Process only `process_date`  
âœ” Overwrite only that partition  
âœ” Idempotent reruns  
âœ” Prevent duplicate records  

Mechanism:

```python
.mode("overwrite")
.option("replaceWhere", "date = 'YYYY-MM-DD'")
```

Industry-standard batch incremental design.

---

---

## â›“ Orchestration Layer â€“ Apache Airflow

Airflow is the **central control plane** of the pipeline.

---

### Airflow Responsibilities

âœ” Schedule pipeline runs  
âœ” Manage task dependencies  
âœ” Trigger AWS Glue Jobs  
âœ” Trigger Glue Crawler  
âœ” Retry & failure handling  

Execution order:

1. API Extraction  
2. Silver Glue Job  
3. Gold Glue Job  
4. Glue Crawler  

---

### Dockerized Airflow Environment

Airflow runs locally inside Docker for:

âœ” Environment reproducibility  
âœ” Dependency isolation  
âœ” Easy configuration  
âœ” Cloud orchestration simulation  

---

### DAG Location & Module Design

Airflow container contains:

âœ” DAG file  
âœ” API client module  
âœ” Extraction logic  
âœ” S3 writer module  

All ingestion modules placed in **same DAG directory** for simplified imports.

Glue jobs run independently on AWS.

---

---

## ğŸ§¾ Metadata & Schema Management

### AWS Glue Crawler

âœ” Infers Parquet schema  
âœ” Detects partitions  
âœ” Updates Data Catalog  

---

### AWS Glue Data Catalog

âœ” Stores table definitions  
âœ” Enables Athena SQL querying  

---

---

## ğŸ” Query Layer â€“ Amazon Athena

Athena enables SQL queries directly on S3 datasets.

---

### Benefits

âœ” No infrastructure management  
âœ” Pay-per-query model  
âœ” Works with Glue Catalog  

---

### Example Queries

```sql
SELECT city, avg_temperature
FROM gold_weather
WHERE date = DATE '2026-02-16';
```

Supports analytics, filtering, aggregations.

---

---

## ğŸ“ˆ BI / Visualization Layer â€“ Power BI

Gold datasets are designed for BI consumption.

---

### Integration Strategy

âœ” Athena used as query backend  
âœ” Power BI connects via Athena connector  
âœ” Enables dashboarding & reporting  

---

### Dashboard Intent

Planned visuals:

âœ” Temperature trends  
âœ” City comparisons  
âœ” KPI metrics  
âœ” Aggregated insights  

(Dashboard implementation planned as future enhancement.)

---

---
---

## ğŸ’° Cost Optimization Strategy

The pipeline is designed with **query efficiency and cost control** in mind.

Since Amazon Athena follows a **pay-per-data-scanned model**, dataset design directly impacts query cost.

Optimizations implemented:

âœ” **Columnar Storage (Parquet)**  
Parquet significantly reduces scan size compared to raw JSON.

âœ” **Partitioning by Date**  
Silver and Gold layers are partitioned by `date`, ensuring Athena scans only relevant partitions.

âœ” **Reduced Dataset Size in Gold Layer**  
Gold layer stores aggregated daily metrics, minimizing query overhead.

Result:

â†’ Faster queries  
â†’ Lower Athena costs  
â†’ Production-aligned data lake design

## âœ… Key Data Engineering Concepts Demonstrated

âœ” Data Lake Layering (Bronze / Silver / Gold)  
âœ” Distributed ETL using Spark  
âœ” Incremental Batch Processing  
âœ” Partition-Aware Design  
âœ” Data Quality Enforcement  
âœ” Cloud-Orchestrated Workflows  
âœ” Metadata-Driven Analytics  

---

---

## ğŸ‘¨â€ğŸ’» Author & Project Context

**Rohit Raj Singh**

This project is part of my professional portfolio and demonstrates a **production-grade cloud data engineering pipeline** using **Apache Airflow and AWS**.

Key skills reflected:

- Workflow orchestration with Apache Airflow (local, Dockerized)  
- REST API ingestion and immutable data lake design  
- AWS Glueâ€“based distributed ETL using PySpark  
- Schema inference and partition management with Glue Crawlers  
- SQL analytics using Amazon Athena  
- Secure AWS integration using boto3 and AWS CLI  
- End-to-end pipeline automation and monitoring  

ğŸ“¬ **LinkedIn:**  
[Connect with me professionally](https://www.linkedin.com/in/rohit-raj-singh-3030172a4?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app)

---
