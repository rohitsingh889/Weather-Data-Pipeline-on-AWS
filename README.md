# ğŸŒ¦ Incremental Weather Data Lake Pipeline on AWS
![Project Overview](https://github.com/rohitsingh889/--weather-lake-pipeline/blob/main/PICS/AWS%20pipeline%20main%20architecture.png)
This project implements a production-style **batch data engineering pipeline** that ingests historical weather data from the **Open-Meteo Archive API**, stores raw data in Amazon S3, performs distributed transformations using **AWS Glue (PySpark)**, generates analytics-ready datasets, and enables SQL-based querying via **AWS Glue Data Catalog and Amazon Athena** â€” fully orchestrated by **Apache Airflow running in a Dockerized local environment**.

---

## ğŸš€ Project Objective

The goal of this pipeline is to demonstrate **real-world cloud data engineering patterns**, including:

âœ” Workflow orchestration  
âœ” API-driven ingestion  
âœ” Multi-layer data lake architecture  
âœ” Distributed Spark transformations  
âœ” Incremental processing strategy  
âœ” Data quality validation  
âœ” Metadata-driven analytics  
âœ” BI consumption workflows  

---

## ğŸ— High-Level Architecture

Pipeline Flow:

Open-Meteo Archive API  
â†’ Python Extraction Layer  
â†’ Amazon S3 (Bronze Layer â€“ Raw JSON)  
â†’ AWS Glue (Silver Layer â€“ PySpark Transformations + Data Quality Checks)  
â†’ Amazon S3 (Silver Layer â€“ Parquet)  
â†’ AWS Glue (Gold Layer â€“ Aggregations)  
â†’ Amazon S3 (Gold Layer â€“ Analytics Ready Parquet)  
â†’ AWS Glue Crawler  
â†’ AWS Glue Data Catalog  
â†’ Amazon Athena  
â†’ BI / Analytics Tools (Power BI)

---

---

## ğŸ”„ End-to-End Data Lifecycle (Step-Wise)

The pipeline follows a structured **Extract â†’ Transform â†’ Load â†’ Analytics** workflow.

---

### 1ï¸âƒ£ Extract Phase â€“ API Ingestion

âœ” Weather data retrieved from Open-Meteo Archive API  
âœ” Python used as ingestion engine  
âœ” REST calls executed via `requests` module  
âœ” Raw JSON responses preserved  

Output:

â†’ Raw JSON stored in Bronze layer (Amazon S3)

---

### 2ï¸âƒ£ Transform Phase â€“ Silver Processing (AWS Glue)

âœ” Partition-specific Bronze data read  
âœ” Nested arrays flattened  
âœ” Timestamps parsed  
âœ” Data types standardized  
âœ” Duplicate records removed  
âœ” Data Quality checks enforced  

Output:

â†’ Cleaned Parquet datasets written to Silver layer

---

### 3ï¸âƒ£ Load Phase â€“ Gold Aggregation (AWS Glue)

âœ” Silver datasets aggregated into daily metrics  
âœ” Business-friendly schema produced  
âœ” Dataset optimized for analytics  

Output:

â†’ Analytics-ready Parquet datasets written to Gold layer

---

### 4ï¸âƒ£ Analytics Phase â€“ Query & BI Consumption

âœ” Glue Crawler infers schema  
âœ” Glue Data Catalog stores metadata  
âœ” Athena executes SQL queries  
âœ” Power BI consumes Gold datasets  

Purpose:

â†’ Business intelligence & reporting

---

---

## ğŸ§© Technologies & Tools Used

| Category | Technology / Tool | Purpose |
|----------|-------------------|---------|
| Orchestration | Apache Airflow (Dockerized) | Workflow scheduling & dependency management |
| Containerization | Docker | Local Airflow environment isolation |
| Language | Python | API ingestion & orchestration logic |
| HTTP Client | Requests Module | REST API communication |
| AWS SDK | Boto3 | Programmatic AWS interaction |
| Cloud Storage | Amazon S3 | Data lake storage layers |
| Distributed Processing | AWS Glue (PySpark) | Spark-based ETL transformations |
| Metadata Discovery | AWS Glue Crawler | Schema inference & partition detection |
| Metadata Catalog | AWS Glue Data Catalog | Athena table definitions |
| Query Engine | Amazon Athena | SQL querying on S3 |
| BI Tool | Power BI | Analytics & visualization |
| Credential Management | AWS CLI + IAM Roles | Secure AWS authentication |

---

---

## ğŸ“¡ Data Source Layer

**Provider:** Open-Meteo Archive API  

The pipeline retrieves **historical hourly weather observations** for configured cities.

Data retrieved includes:

- temperature_2m  
- precipitation  
- windspeed_10m  
- timestamp  

The API returns nested JSON structures requiring flattening.

---

---

## âš™ Extraction Layer (Python)

### Responsibilities

âœ” Call Open-Meteo API using `requests`  
âœ” Fetch previous day's data dynamically  
âœ” Preserve raw JSON response  
âœ” Upload directly to S3 Bronze layer using `boto3`

---

### Libraries Used

**Requests Module**

Used for external REST API communication.

**Boto3 (AWS SDK)**

Used for:

âœ” S3 PutObject operations  
âœ” IAM-based authentication  
âœ” Secure AWS integration  

No AWS keys hardcoded.

---

### AWS Credential Verification

Authentication handled via:

âœ” AWS CLI configuration  
âœ” IAM user / role permissions  
âœ” Boto3 credential provider chain  

Driver logic:

- Boto3 automatically resolves credentials  
- Uses environment / IAM / config chain  
- No manual secret handling required  

---

---

## ğŸ—‚ Bronze Layer â€“ Raw Data Zone

**Storage:** Amazon S3  
**Format:** Raw JSON  
**Partitioning Strategy:**

```
bronze/weather/
    city=XYZ/
        year=YYYY/
            month=MM/
                day=DD/
```

---

### Purpose

âœ” Immutable raw storage  
âœ” Replay capability  
âœ” Debugging & auditing  
âœ” Schema recovery  

No transformations applied.

---

---

## ğŸ”„ Silver Layer â€“ Transformation & Validation Zone

**Processing Engine:** AWS Glue (PySpark)

---

### Responsibilities

âœ” Read partition-specific Bronze JSON  
âœ” Flatten nested hourly arrays  
âœ” Parse timestamps  
âœ” Cast numeric fields  
âœ” Remove duplicates  
âœ” Apply Data Quality Checks  

---

### Output Format

âœ” Parquet (Columnar, optimized)

Partitioning:

```
silver/weather/date=YYYY-MM-DD/
```

---

### Data Quality Strategy

Implemented directly in Spark:

âœ” Null validation  
âœ” Domain range validation  
âœ” Duplicate detection  
âœ” Fail-fast enforcement  

Example checks:

- Null timestamps rejected  
- Temperature bounds enforced  
- Negative windspeed prevented  
- Duplicate city/timestamp removed  

Bad data â†’ Job fails intentionally.

---

### ğŸš¦ Data Quality Enforcement Strategy

Silver layer transformations implement a **fail-fast validation model**:

âœ” Null checks  
âœ” Domain range checks  
âœ” Duplicate detection  

Invalid records trigger controlled job failure to prevent downstream corruption.

**Future Enhancement â€“ Quarantine Pattern**

Planned extension:

âœ” Redirect invalid rows to `silver/quarantine/`  
âœ” Enable root-cause analysis  
âœ” Preserve pipeline continuity  

---

## ğŸ“Š Gold Layer â€“ Analytics Zone

**Processing Engine:** AWS Glue (Aggregation Job)

---

### Responsibilities

Transform Silver hourly records â†’ Daily metrics

Aggregations:

- avg_temperature  
- max_temperature  
- total_precipitation  
- avg_windspeed  

---

### Output

âœ” Parquet  
âœ” Partitioned by date  

```
gold/weather/date=YYYY-MM-DD/
```

---

### Purpose

âœ” BI-ready datasets  
âœ” Reduced scan cost  
âœ” Faster Athena queries  

---

---

## ğŸ” Incremental Processing Strategy

The pipeline follows a **partition-level incremental model**.

Behavior:

âœ” Process only `process_date`  
âœ” Overwrite only that partition  
âœ” Idempotent reruns  
âœ” Prevent duplicate records  

Mechanism:

```python
.mode("overwrite")
.option("replaceWhere", "date = 'YYYY-MM-DD'")
```

---

---

## â›“ Orchestration Layer â€“ Apache Airflow

Airflow is the **central control plane** of the pipeline.

---

### Airflow Responsibilities

âœ” Schedule pipeline runs  
âœ” Manage task dependencies  
âœ” Trigger AWS Glue Jobs  
âœ” Trigger Glue Crawler  
âœ” Retry & failure handling  

---

### Dockerized Airflow Environment

âœ” Local reproducible setup  
âœ” Containerized execution  
âœ” Cloud orchestration simulation  

---

---

## ğŸ§¾ Metadata & Schema Management

âœ” Glue Crawler infers schema  
âœ” Glue Data Catalog stores tables  

---

---

## ğŸ” Querying Gold Data with Amazon Athena

Amazon Athena enables **SQL querying directly on S3 Parquet datasets**.

---

### Why Athena?

âœ” No infrastructure management  
âœ” Pay-per-query pricing  
âœ” Glue Catalog integration  

---

### Example Queries

```sql
SELECT city, avg_temperature
FROM gold_weather
WHERE date = DATE '2026-02-16';
```

---

### Common Analytics Queries

```sql
-- Hottest city
SELECT city, max_temperature
FROM gold_weather
ORDER BY max_temperature DESC
LIMIT 1;

-- Daily trend
SELECT date, avg_temperature
FROM gold_weather
WHERE city = 'Delhi'
ORDER BY date;
```

---

---

## ğŸ“ˆ BI / Visualization Layer â€“ Power BI

âœ” Athena used as SQL backend  
âœ” Gold datasets optimized for dashboards  

Planned visuals:

âœ” Temperature trends  
âœ” City comparisons  
âœ” KPI metrics  

---

---

## ğŸ’° Cost Optimization Strategy

âœ” Parquet columnar format  
âœ” Partitioned datasets  
âœ” Reduced Athena scan costs  

---

---

## ğŸ‘¨â€ğŸ’» Author & Project Context

**Rohit Raj Singh**

This project is part of my professional portfolio and demonstrates a **production-grade cloud data engineering pipeline** using **Apache Airflow and AWS**.

Key skills reflected:

- Workflow orchestration with Apache Airflow (local, Dockerized)  
- REST API ingestion and immutable data lake design  
- AWS Glueâ€“based distributed ETL using PySpark  
- Schema inference and partition management with Glue Crawlers  
- SQL analytics using Amazon Athena  
- Secure AWS integration using boto3 and AWS CLI  
- End-to-end pipeline automation and monitoring  

ğŸ“¬ **LinkedIn:**  
[Connect with me professionally](https://www.linkedin.com/in/rohit-raj-singh-3030172a4?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app)

---
